prometheus:
  prometheusSpec:
    retention: 15d
    resources:
      requests:
        memory: 400Mi
        cpu: 250m
      limits:
        memory: 1Gi
        cpu: 500m
  # Enable scraping of kubelet and cAdvisor natively
  kubelet:
    enabled: true
  kubeControllerManager:
    enabled: false
  kubeScheduler:
    enabled: false
  kubeProxy:
    enabled: false

grafana:
  enabled: true
  adminPassword: "admin" # Change this in production or use an auto-generated secret
  additionalDataSources:
    - name: Loki
      type: loki
      url: http://loki-stack.logging.svc.cluster.local:3100
      access: proxy
  service:
    type: LoadBalancer
    port: 80
    targetPort: 3000
    annotations:
      tailscale.com/hostname: "devops-monitoring"
    loadBalancerClass: "tailscale"
  resources:
    requests:
      memory: 100Mi
      cpu: 100m
    limits:
      memory: 200Mi
      cpu: 200m

alertmanager:
  enabled: true
  service:
    type: ClusterIP
    port: 9093

  config:
    global:
      resolve_timeout: 5m
    route:
      group_by: ['alertname', 'job']
      group_wait: 30s
      group_interval: 5m
      repeat_interval: 1h
      receiver: 'webhook-receiver'
    receivers:
    - name: 'webhook-receiver'
      webhook_configs:
      - url: 'https://webhook.site/placeholder-url-for-demo' # Dummy URL for demonstration
  alertmanagerSpec:
    resources:
      requests:
        memory: 50Mi
        cpu: 50m
      limits:
        memory: 100Mi
        cpu: 100m

additionalPrometheusRulesMap:
  custom-rules:
    groups:
      - name: custom_alerts
        rules:
          - alert: InstanceDown
            expr: up == 0
            for: 1m
            labels:
              severity: critical
            annotations:
              summary: "Instance {{ $labels.instance }} down"
              description: "{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 1 minute."
          - alert: HighCPUUsage
            expr: sum(rate(container_cpu_usage_seconds_total{container!=""}[5m])) by (pod) > 0.8
            for: 2m
            labels:
              severity: warning
            annotations:
              summary: "High CPU usage on pod {{ $labels.pod }}"
              description: "Pod {{ $labels.pod }} is using over 80% CPU."
          - alert: PodCrashLooping
            expr: rate(kube_pod_container_status_restarts_total[5m]) * 300 > 2
            for: 2m
            labels:
              severity: critical
            annotations:
              summary: "Pod {{ $labels.pod }} is crash looping"
              description: "Pod {{ $labels.pod }} is successfully restarting too frequently."

extraManifests:
  - apiVersion: v1
    kind: Service
    metadata:
      name: alertmanager-tailscale
      namespace: monitoring
      annotations:
        tailscale.com/hostname: "devops-alerts"
    spec:
      type: LoadBalancer
      loadBalancerClass: tailscale
      selector:
        alertmanager: kube-prometheus-stack-alertmanager
        app.kubernetes.io/name: alertmanager
      ports:
        - name: web
          port: 80
          targetPort: 9093
